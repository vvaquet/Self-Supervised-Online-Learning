{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# Disable warnings\n",
    "np.seterr(divide='ignore')\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Functions to evaluate models\n",
    "\n",
    "def eval_model(model, Xs, ys, certainty_threshold):\n",
    "    accuracies = []\n",
    "    num_updated = []\n",
    "    for X, y in zip(Xs, ys):\n",
    "        cur_model = model(X[:warm_start_cutoff], y[:warm_start_cutoff], k, window_size, certainty_threshold=certainty_threshold)\n",
    "    \n",
    "        preds = []\n",
    "        num_updates = 0\n",
    "        for i in range(warm_start_cutoff, y.shape[0]):\n",
    "            pred, added = cur_model.predict_update(X[i], y[i])\n",
    "            preds.append(pred)\n",
    "            if added :\n",
    "                num_updates +=1 \n",
    "\n",
    "        accuracies.append(sklearn.metrics.accuracy_score(np.array(preds), y[warm_start_cutoff:]))\n",
    "        num_updated.append(num_updates)\n",
    "    \n",
    "    return np.array(accuracies), np.array(num_updated)\n",
    "\n",
    "\n",
    "def eval_baseline(model, Xs, ys):\n",
    "    accuracies = []\n",
    "    for X, y in zip(Xs, ys):\n",
    "        cur_model = model(X[:warm_start_cutoff], y[:warm_start_cutoff], k, window_size)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(warm_start_cutoff, y.shape[0]):\n",
    "            preds.append(cur_model.predict_update(X[i], y[i]))\n",
    "\n",
    "        accuracies.append(sklearn.metrics.accuracy_score(np.array(preds), y[warm_start_cutoff:]))\n",
    "    \n",
    "    return np.array(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.KnnC2 import KnnUnsupervisedC2\n",
    "from models.KnnC1 import KnnUnsupervisedC1\n",
    "from models.KnnC3 import KnnUnsupervisedC3\n",
    "\n",
    "# functions for finding the optimal thresholds\n",
    "\n",
    "def eval_thresholds(Xs, y, thresholds1, thresholds2, thresholds3):\n",
    "    all_accs_thres3 = []\n",
    "    accs_c1 = []\n",
    "    accs_c3 = []\n",
    "    num_updates_c2 = []\n",
    "    num_updates_c1 = []\n",
    "    num_updates_c3 = []\n",
    "\n",
    "    for thres in thresholds2:\n",
    "        acc_c1, num_updates = eval_model(KnnUnsupervisedC1, Xs, y, thres)\n",
    "        print(f'{thres}:      {np.mean(acc_c1):.3f} +- {np.std(acc_c1):.3f}')\n",
    "        accs_c1.append(acc_c1)\n",
    "        num_updates_c1\n",
    "\n",
    "    for thres in thresholds1:\n",
    "        acc_c2, num_updates = eval_model(KnnUnsupervisedC2, Xs, y, thres)\n",
    "        print(f'{thres}:     {np.mean(acc_c2):.3f} +- {np.std(acc_c2):.3f}')\n",
    "        all_accs_thres3.append(acc_c2)\n",
    "        num_updates_c2.append(num_updates)\n",
    "\n",
    "    for thres in thresholds3:\n",
    "        acc_c3, num_updates = eval_model(KnnUnsupervisedC3, Xs, y, thres)\n",
    "        print(f'{thres}:      {np.mean(acc_c3):.3f} +- {np.std(acc_c3):.3f}')\n",
    "        accs_c3.append(acc_c3)\n",
    "        num_updates_c3.append(num_updates)\n",
    "\n",
    "    \n",
    "    return all_accs_thres3, accs_c1, accs_c3, num_updates_c2, num_updates_c1, num_updates_c3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_models.supervised import KnnSupervised\n",
    "from baseline_models.unsupervised_upper_bound import KnnUnsupervisedUpperBound\n",
    "from baseline_models.unsupervised_naive import KnnUnsupervisedNaive\n",
    "from baseline_models.unsupervised_naive_nolearn import KnnUnsupervisedNaiveNoLearn\n",
    "from models.KnnC1 import KnnUnsupervisedC1\n",
    "from models.KnnC2 import KnnUnsupervisedC2\n",
    "from models.KnnC3 import KnnUnsupervisedC3\n",
    "\n",
    "\n",
    "# experimental function\n",
    "\n",
    "def dataset_experiment(Xs, y, threshold_basis=None, threshold_neighbor1=None, threshold_neighbor2=None, f_out=None):\n",
    "    print(len(Xs))\n",
    "\n",
    "    out = 'method,mean,std\\n'\n",
    "    acc_baseline_supervised = eval_baseline(KnnSupervised, Xs, y)\n",
    "    print(f'Supervised:  {np.mean(acc_baseline_supervised):.3f} +- {np.std(acc_baseline_supervised):.3f}')\n",
    "    out += 'Supervised,' + str(np.mean(acc_baseline_supervised)) + ',' + str(np.std(acc_baseline_supervised)) + '\\n'\n",
    "\n",
    "    acc_baseline_upper_bound = eval_baseline(KnnUnsupervisedUpperBound, Xs, y)\n",
    "    print(f'Upper Bound: {np.mean(acc_baseline_upper_bound):.3f} +- {np.std(acc_baseline_upper_bound):.3f}')\n",
    "    out += 'Perfect Policy,' + str(np.mean(acc_baseline_upper_bound)) + ',' + str(np.std(acc_baseline_upper_bound)) + '\\n'\n",
    "\n",
    "    acc_baseline_naive = eval_baseline(KnnUnsupervisedNaive, Xs, y)\n",
    "    print(f'Naive:       {np.mean(acc_baseline_naive):.3f} +- {np.std(acc_baseline_naive):.3f}')\n",
    "    out += 'Naive,' + str(np.mean(acc_baseline_naive)) + ',' + str(np.std(acc_baseline_naive)) + '\\n'\n",
    "\n",
    "    acc_baseline_nolearn = eval_baseline(KnnUnsupervisedNaiveNoLearn, Xs, y)\n",
    "    print(f'No Learn:    {np.mean(acc_baseline_nolearn):.3f} +- {np.std(acc_baseline_nolearn):.3f}')\n",
    "    out += 'No Learn,' + str(np.mean(acc_baseline_nolearn)) + ',' + str(np.std(acc_baseline_nolearn)) + '\\n'\n",
    "\n",
    "    \n",
    "    acc_c1, _ = eval_model(KnnUnsupervisedC1, Xs, y, threshold_neighbor1)\n",
    "    print(f'c1:      {np.mean(acc_c1):.3f} +- {np.std(acc_c1):.3f}')\n",
    "    out += 'c1,' + str(np.mean(acc_c1)) + ',' + str(np.std(acc_c1)) + '\\n'\n",
    "    \n",
    "    acc_c2, _ = eval_model(KnnUnsupervisedC2, Xs, y, threshold_basis)\n",
    "    print(f'c2:      {np.mean(acc_c2):.3f} +- {np.std(acc_c2):.3f}')\n",
    "    out += 'c2,' + str(np.mean(acc_c2)) + ',' + str(np.std(acc_c2)) + '\\n'\n",
    "\n",
    "    acc_c3, _ = eval_model(KnnUnsupervisedC3, Xs, y, threshold_neighbor2)\n",
    "    print(f'c3:      {np.mean(acc_c3):.3f} +- {np.std(acc_c3):.3f}')\n",
    "    out += 'c3,' + str(np.mean(acc_c3)) + ',' + str(np.std(acc_c3)) + '\\n'\n",
    "    \n",
    "    if f_out is not None:\n",
    "        file = open(f_out, \"w\") \n",
    "        file.write(out)\n",
    "        file.close() \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Leaky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experimental parameters\n",
    "warm_start_cutoff = 50\n",
    "window_size = 50\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = np.load('Leaky.npz')\n",
    "Xs = dataset['Xs']\n",
    "ys = dataset['ys']\n",
    "print(Xs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments with automatically determined threshold\n",
    "dataset_experiment(Xs, ys, f_out='results_leaky.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal threshold\n",
    "leaky_basis3, leaky_basis4, leaky_basis5, leaky_updates_thres3, leaky_updates_thres4, leaky_updates_thres5 = eval_thresholds(Xs, ys, np.linspace(0,1,41), np.linspace(-1,1,81), np.linspace(-1,1,81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky3 = np.vstack(leaky_basis3)\n",
    "print(np.mean(np.max(leaky3, axis=0)), np.std(np.max(leaky3, axis=0)))\n",
    "\n",
    "leaky4 = np.vstack(leaky_basis4)\n",
    "print(np.mean(np.max(leaky4, axis=0)), np.std(np.max(leaky4, axis=0)))\n",
    "\n",
    "leaky5 = np.vstack(leaky_basis5)\n",
    "print(np.mean(np.max(leaky5, axis=0)), np.std(np.max(leaky5, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(leaky3.shape[0]):\n",
    "    plt.plot(np.linspace(-1,1,81),leaky4[:,i], c='tab:blue', alpha=0.3, label='c1')\n",
    "\n",
    "    plt.plot(np.linspace(0,1,41),leaky3[:,i], c='tab:orange', alpha=0.3, label='c2')\n",
    "    plt.plot(np.linspace(-1,1,81),leaky5[:,i], c='tab:green', alpha=0.3, label='c3')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Theta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_cutoff = 100\n",
    "window_size = 100\n",
    "k = 5\n",
    "\n",
    "# unfortunately the HSI dataset cannot be made publically available, the experimental code for the experiment is the same as for Leaky"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
